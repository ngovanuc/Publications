# -*- coding: utf-8 -*-
"""Bản gốc - Áp dụng Machine learning để phát hiện mã độc trong IoT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AFCQ37HY6WgKoM6WfwykcJg79_LgF6K

## Set up: Environment and Library
"""

!apt-get update -qq 2>&1 > /dev/null
!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
!apt-get update

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

from google.colab import drive
drive.mount('/content/drive')

import datetime

import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
from collections import defaultdict
import math
from scipy.stats import skew
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn import preprocessing

# Commented out IPython magic to ensure Python compatibility.
# import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB


from sklearn.model_selection import GridSearchCV
from sklearn.inspection import permutation_importance
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc
import time
# %matplotlib inline

from sklearn.datasets import make_classification

from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import label_binarize

from sklearn import metrics

pd.options.display.max_rows = 300
pd.options.display.max_columns = 300

"""# 1 Create dataset direct and save to csv"""

capture_34 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-34-1/bro/conn.log.labeled"
capture_43 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-43-1/bro/conn.log.labeled"
capture_44 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-44-1/bro/conn.log.labeled"
capture_49 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-49-1/bro/conn.log.labeled"
capture_52 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-52-1/bro/conn.log.labeled"
capture_20 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-20-1/bro/conn.log.labeled"
capture_21 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-21-1/bro/conn.log.labeled"
capture_42 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-42-1/bro/conn.log.labeled"
capture_60 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-60-1/bro/conn.log.labeled"
capture_17 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-17-1/bro/conn.log.labeled"
capture_36 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-36-1/bro/conn.log.labeled"
capture_33 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-33-1/bro/conn.log.labeled"
capture_8 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-8-1/bro/conn.log.labeled"
capture_35 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-35-1/bro/conn.log.labeled"
capture_48 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-48-1/bro/conn.log.labeled"
capture_39 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-39-1/bro/conn.log.labeled"
capture_7 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-7-1/bro/conn.log.labeled"
capture_9 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-9-1/bro/conn.log.labeled"
capture_3 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-3-1/bro/conn.log.labeled"
capture_1 = "https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-1-1/bro/conn.log.labeled"

"""###Mirai(34, 43, 44, 49, 52, 35, 48)"""

df34 = pd.read_table(filepath_or_buffer=capture_34, skiprows=10, nrows=200000)
df34.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df34.drop(df34.tail(1).index,inplace=True)
df34.shape[0]
df34['label'].value_counts()

df35 = pd.read_table(filepath_or_buffer=capture_35, skiprows=10, nrows=400000)
df35.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df35.drop(df35.tail(1).index,inplace=True)
df35.shape[0]
df35['label'].value_counts()

df44 = pd.read_table(filepath_or_buffer=capture_44, skiprows=10, nrows=200000)
df44.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df44.drop(df44.tail(1).index,inplace=True)
df44['label'].value_counts()

df48 = pd.read_table(filepath_or_buffer=capture_48, skiprows=10, nrows=200000)
df48.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df48.drop(df48.tail(1).index,inplace=True)
df48['label'].value_counts()

df49 = pd.read_table(filepath_or_buffer=capture_49, skiprows=10, nrows=200000)
df49.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df49.drop(df49.tail(1).index,inplace=True)
df48['label'].value_counts()

"""###Torii(20, 21)"""

df20 = pd.read_table(filepath_or_buffer=capture_20, skiprows=10, nrows=200000)
df20.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df20.drop(df20.tail(1).index,inplace=True)
df20.shape[0]
df20['label'].value_counts()

df21 = pd.read_table(filepath_or_buffer=capture_21, skiprows=10, nrows=200000)
df21.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df21.drop(df21.tail(1).index,inplace=True)
df21.shape[0]
df21['label'].value_counts()

"""###Trojan (42)"""

df42 = pd.read_table(filepath_or_buffer=capture_42, skiprows=10, nrows=200000)
df42.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df42.drop(df42.tail(1).index,inplace=True)
df42['label'].value_counts()

"""###Gagfyt(60)"""

df60 = pd.read_table(filepath_or_buffer=capture_60, skiprows=10,nrows=500000)
df60.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df60.drop(df60.tail(1).index,inplace=True)
df60['label'].value_counts()

"""###Kenjiro(17, 33)

###Okini(36)
"""

df36 = pd.read_table(filepath_or_buffer=capture_36, skiprows=10, nrows=500000)
df36.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df36.drop(df36.tail(1).index,inplace=True)
df36['label'].value_counts()

"""###Hakai(8)"""

df8 = pd.read_table(filepath_or_buffer=capture_8, skiprows=10)
df8.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df8.drop(df8.tail(1).index,inplace=True)
df8.shape
df8['label'].value_counts()

"""###Linux-Mirai(7)"""

df7 = pd.read_table(filepath_or_buffer=capture_7, skiprows=10, nrows=300000)
df7.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df7.drop(df7.tail(1).index,inplace=True)
df7['label'].value_counts()

"""###Linux-Hajime(9)"""

df9 = pd.read_table(filepath_or_buffer=capture_9, skiprows=10, nrows=300000)
df9.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df9.drop(df9.tail(1).index,inplace=True)
df9['label'].value_counts()

"""###Muhstik(3)"""

df3 = pd.read_table(filepath_or_buffer=capture_3, skiprows=10)
df3.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df3.drop(df3.tail(1).index,inplace=True)
df3['label'].value_counts()

"""### Hide&Seek(1)"""

df1 = pd.read_table(filepath_or_buffer=capture_1, skiprows=10, nrows=200000)
df1.columns=['ts',
              'uid',
              'id.orig_h',
              'id.orig_p',
              'id.resp_h',
              'id.resp_p',
              'proto',
              'service',
              'duration',
              'orig_bytes',
              'resp_bytes',
              'conn_state',
              'local_orig',
              'local_resp',
              'missed_bytes',
              'history',
              'orig_pkts',
              'orig_ip_bytes',
              'resp_pkts',
              'resp_ip_bytes',
              'label']
df1.drop(df1.tail(1).index,inplace=True)
df1['label'].value_counts()

"""## 1.1.2 Store Data"""

frames=[df1, df20, df3, df35, df34, df36, df42, df44, df60, df7, df8, df9, df48, df49, df21]

df_preprocess=pd.concat(frames)

df_preprocess.info()

df_preprocess['label'].value_counts()

df_preprocess.head()

df_preprocess.loc[(df_preprocess.label == '-   Malicious   PartOfAHorizontalPortScan'), 'label'] = 'PartOfAHorizontalPortScan'
df_preprocess.loc[(df_preprocess.label == '(empty)   Malicious   PartOfAHorizontalPortScan'), 'label'] = 'PartOfAHorizontalPortScan'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   Okiru'), 'label'] = 'Okiru'
df_preprocess.loc[(df_preprocess.label == '(empty)   Malicious   Okiru'), 'label'] = 'Okiru'
df_preprocess.loc[(df_preprocess.label == '-   Benign   -'), 'label'] = 'Benign'
df_preprocess.loc[(df_preprocess.label == '(empty)   Benign   -'), 'label'] = 'Benign'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   DDoS'), 'label'] = 'DDoS'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C'), 'label'] = 'C&C'
df_preprocess.loc[(df_preprocess.label == '(empty)   Malicious   C&C'), 'label'] = 'C&C'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   Attack'), 'label'] = 'Attack'
df_preprocess.loc[(df_preprocess.label == '(empty)   Malicious   Attack'), 'label'] = 'Attack'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C-HeartBeat'), 'label'] = 'C&C-HeartBeat'
df_preprocess.loc[(df_preprocess.label == '(empty)   Malicious   C&C-HeartBeat'), 'label'] = 'C&C-HeartBeat'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C-FileDownload'), 'label'] = 'C&C-FileDownload'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C-Torii'), 'label'] = 'C&C-Torii'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C-HeartBeat-FileDownload'), 'label'] = 'C&C-HeartBeat-FileDownload'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   FileDownload'), 'label'] = 'FileDownload'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   C&C-Mirai'), 'label'] = 'C&C-Mirai'
df_preprocess.loc[(df_preprocess.label == '-   Malicious   Okiru-Attack'), 'label'] = 'Okiru-Attack'

df_preprocess['label'].value_counts()

df_preprocess['label'].value_counts()

df_preprocess['label'].value_counts().plot(kind='bar')

df_preprocess

"""## Write data to csv"""

# xuong duoi phan sau xu li file
from google.colab import files
df_preprocess.to_csv('Runtime_data.csv')
files.download('Runtime_data.csv')

"""# 1.1' READ DIRECT DATA FROM CSV
to reduce time to read data from source and limit data

# Read data csv
"""

# link download dataset Runtime_data.csv : https://drive.google.com/file/d/1VLW4aMrRyfptpA3wpK04Imbt_xHbSVKY/view?usp=share_link
df_dataset = pd.read_csv("/content/drive/MyDrive/DATN_TAILIEUDOC_IOT_ML/Data_Code/dataset/Runtime_data.csv", low_memory=False)

df_dataset

# horizontal_concat.drop(columns=['Unnamed'], axis=0)
df_dataset.info()

df_dataset = df_dataset.drop('Unnamed: 0', axis=1)

df_dataset['label'].value_counts()

"""# 2.Tiền xử lý - trực quan data"""

df_preprocess = df_dataset

"""## label dataset for detect malware or bengin
(các nhãn là mã độc sẽ được đánh là 1 còn trạng thái bình thường là 0)
"""

df_preprocess.loc[(df_preprocess.label == 'PartOfAHorizontalPortScan'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'Okiru'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'Benign'), 'label'] = 0
df_preprocess.loc[(df_preprocess.label == 'DDoS'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'Attack'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C-HeartBeat'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C-FileDownload'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C-Torii'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C-HeartBeat-FileDownload'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'FileDownload'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'C&C-Mirai'), 'label'] = 1
df_preprocess.loc[(df_preprocess.label == 'Okiru-Attack'), 'label'] = 1

df_preprocess['label'].value_counts()

df_preprocess.dtypes

df_preprocess["label"] = df_preprocess["label"].astype(str).astype(float)

df_preprocess

df_visual = df_preprocess

df_visual.info()

df_visual

df_visual.describe()

# show dtype of dataset

fig, axes = plt.subplots(1, 2, sharex=False, figsize=(20,5))
m = df_visual.dtypes.value_counts()
print(m)
labels = ['Object(Category)', 'Float']
sizes = m
explode = (0, 0.1) # only "explode" the 2nd slice (i.e. 'Hogs')
axes[0].pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)
axes[0].set_title('Dtype Percentage(%)')
axes[1].bar(labels, m, color=['b','orange','g'])
axes[1].set_title('Dtype of dataset Columns')

df_visual.isna().sum()

# Split into numerical columns and non-numberic(object) columns
cat_cols = []
for col in df_visual.select_dtypes(include='object'):
  cat_cols.append(col)
num_cols = []
for col in df_visual.select_dtypes(exclude='object'):
  num_cols.append(col)
print(num_cols)
print('----')
print(cat_cols)

# plot
x = ['Category Columns', 'Number Columns']
y = [len(cat_cols), len(num_cols)]
plt.bar(x, y, color=['red', 'green'])

"""## 1. Object columns"""

# count values in one columns
for col in cat_cols:
  print(df_visual[col].value_counts())
  print("\n=========")

"""## Split to small category for object type"""

cat_cols

need_remove_cols = ['ts', 'uid','duration','local_orig', 'orig_bytes', 'local_resp', 'label', 'id.orig_h', 'id.resp_h']
new_cat_cols = [col for col in cat_cols if (col not in need_remove_cols)]
print(new_cat_cols)

for col in new_cat_cols:
  print(df_visual[col].value_counts())
  print("\n============================")

# COUNTPLOT
sns.set(font_scale=1)
plt.rcParams["figure.figsize"] = (35,6)
for i in new_cat_cols:
        categories = df_visual[i].unique()
        sns.countplot(x=i,data=df_visual,)
        plt.title(i)
        plt.show()

"""## 2. Numberic columns

Mô tả các cột
"""

df_visual.describe()

for col in num_cols:
  print(df_visual[col].value_counts().sort_index())
  print("\n=========")

# violin plot
fig, axes = plt.subplots(2,4, figsize=(20, 10), sharey=True);
fig.suptitle('Violin Plot', fontsize=20);
num_cols1 = num_cols.copy()
corr = df_visual[num_cols1].corr()
for i,col in zip(range(7),corr):
      sns.violinplot(x=df_visual[col],ax=axes[i//4][i%4])

# boxplot
fig, axes = plt.subplots(2,4, figsize=(20, 10), sharey=True);
fig.suptitle('Box Plot', fontsize=20);
num_cols1 = num_cols.copy()
corr = df_visual[num_cols1].corr()
for i,col in zip(range(7),corr):
      sns.boxplot(x=df_visual[col],ax=axes[i//4][i%4])

df_visual.shape

df_visual.fillna(-1,inplace=True)
df_visual.isna().sum()

"""# 3.Process Data """

df_visual

# df_process_raw = df_visual
df_process = df_visual

df_process.info()

df_process = df_visual.drop(columns=['uid','id.orig_h','id.resp_h','local_orig','local_resp'])

"""## 3.1 Convert to nummeric value and balace data

### Balance data
(vì bài toán đầu tiên là bài toán nhị phân xác định mạng có nhiễm mã độc botnet hay không -> data cũ có số lượng dữ liệu cho mã độc gấp 4 lần dữ liệu cần bằng để tránh hiện tượng imbalanced ảnh hưởng kết quả thuật toán)
"""

# fraction of rows

# here we get 30 % row from the df_process
# due to balance with amount of bengin data

# make malware dataframe put into another dataframe df1

labels = [0, 1]
df_balance =[]

df_malware = df_process[df_process.label == 1]
df_bengin = df_process[df_process.label == 0]

df_malware_balance = df_malware.sample(frac=.4)

df_balance.append(df_bengin)
df_balance.append(df_malware_balance)

df_balance = pd.concat(df_balance)

# shuffle the DataFrame rows
from sklearn.utils import shuffle
df_balance = shuffle(df_balance)

df_balance.tail()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df_balance['proto'] = df_balance['proto'].str.replace('-', ' ')
df_balance['proto'] = df_balance['proto'].astype(str)
df_balance['proto'] = encoder.fit_transform(df_balance['proto'])

df_balance['service'] = df_balance['service'].str.replace('-', ' ')
df_balance['service'] = df_balance['service'].astype(str)
df_balance['service'] = encoder.fit_transform(df_balance['service'])

df_balance['conn_state'] = df_balance['conn_state'].str.replace('-', ' ')
df_balance['conn_state'] = df_balance['conn_state'].astype(str)
df_balance['conn_state'] = encoder.fit_transform(df_balance['conn_state'])

df_balance['history'] = df_balance['history'].str.replace('-', ' ')
df_balance['history'] = df_balance['history'].astype(str)
df_balance['history'] = encoder.fit_transform(df_balance['history'])

df_balance['duration'] = df_balance['duration'].str.replace('S0', ' ')
df_balance['duration'] = df_balance['duration'].str.replace('-', ' ')
df_balance['duration'] = pd.to_numeric(df_balance['duration'], errors='coerce')
df_balance['duration'] = df_balance['duration'].astype(str).astype(float)

# df_balance['orig_bytes'] = df_balance['orig_bytes'].str.replace('-', ' ')
df_balance['orig_bytes'] = pd.to_numeric(df_balance['orig_bytes'], errors='coerce')

df_balance['resp_bytes'] = df_balance['resp_bytes'].str.replace('-', ' ')
df_balance['resp_bytes'] = pd.to_numeric(df_balance['resp_bytes'], errors='coerce')


df_balance[['duration','orig_bytes','resp_bytes']] = df_balance[['duration','orig_bytes','resp_bytes']].fillna(value=df_balance[['duration','orig_bytes','resp_bytes']].mean())

"""## 3.2 correlation feature"""

df_balance

df_balance.info()

# df_visual.hist(figsize = (40, 40))
#nen convert ve numberic roi plot
df_balance.hist(alpha=0.6, figsize=(21 + 1, 21 + 1), color='green')
plt.savefig("dataset.png")
plt.show()

#decribe data
df_balance.describe()

# Mô ta su tuong quan giua cac dac trung trong data da duoc lam sach
corr = df_balance.corr()
columns_count = len(corr.columns)
fig, ax = plt.subplots(figsize=[columns_count, columns_count])
corr = corr.abs()
sns.heatmap(corr, annot=True, fmt='.0%', cmap='Greens', ax=ax)

#get column
columns = list(df_balance.columns)
print(columns)

"""# 3.2 Feature Selection

"""

df_feature = df_balance

df_feature

cat_cols = []
for col in df_feature.select_dtypes(include='object'):
  cat_cols.append(col)
num_cols = []
for col in df_feature.select_dtypes(exclude='object'):
  num_cols.append(col)
print(num_cols)
print('----')
print(cat_cols)

cat_cols.append("label")

X = df_feature.drop(columns=cat_cols, axis=1)
Y = df_feature['label']

X.head()

X.info()

Y.head()

from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)

# parameters to be tested - Iterate several times to find better ranges
# co kq - k can chay lai
# parameters = {
#     'n_estimators': [50,200,15, 150],
#     'min_samples_split': [1, 5 , 10 ,20, 25, 30],
#     'min_samples_leaf': [2, 5, 8, 10],
#     'max_depth': [3,4, 5, 7, 9]
# }

# Khoi tao model
# model = RandomForestClassifier(random_state=42)
# grid search using cv
# clf = GridSearchCV(model, parameters)
# clf.fit(X_train, y_train)
# print("Accuracy on test data: {:.3f}".format(clf.score(X_test, y_test)))

# clf.best_estimator_

clf = RandomForestClassifier(max_depth=5, min_samples_leaf=2,
                               min_samples_split=10, random_state=42)
clf.fit(X_train, y_train)
print("Accuracy on test data: {:.3f}".format(clf.score(X_test, y_test)))

from sklearn.inspection import permutation_importance
result = permutation_importance(clf, X_train, y_train, n_repeats=2, random_state=42, n_jobs=2
)
perm_sorted_idx = result.importances_mean.argsort()

tree_importance_sorted_idx = np.argsort(clf.feature_importances_)
tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))
ax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)
ax1.set_yticks(tree_indices)
ax1.set_yticklabels(X.columns[tree_importance_sorted_idx])
ax1.set_ylim(0, (len(clf.feature_importances_)))
ax2.boxplot(
    result.importances[perm_sorted_idx].T,
    vert=False,
    labels=X.columns[perm_sorted_idx],
)
fig.tight_layout()
plt.show()

best_score = ['ts', 'id.resp_p', 'orig_ip_bytes' ,'id.orig_p', 'duration', 'proto', "orig_pkts", 'history', "conn_state"]

from scipy.stats import spearmanr
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))
corr = spearmanr(X).correlation

# Ensure the correlation matrix is symmetric
corr = (corr + corr.T) / 2
np.fill_diagonal(corr, 1)

# We convert the correlation matrix to a distance matrix before performing
# hierarchical clustering using Ward's linkage.
distance_matrix = 1 - np.abs(corr)
dist_linkage = hierarchy.ward(squareform(distance_matrix))
dendro = hierarchy.dendrogram(
    dist_linkage, labels=X.columns.tolist(), ax=ax1, leaf_rotation=90
)
dendro_idx = np.arange(0, len(dendro["ivl"]))

ax2.imshow(corr[dendro["leaves"], :][:, dendro["leaves"]])
ax2.set_xticks(dendro_idx)
ax2.set_yticks(dendro_idx)
ax2.set_xticklabels(dendro["ivl"], rotation="vertical")
ax2.set_yticklabels(dendro["ivl"])
fig.tight_layout()
plt.show()

"""## Visual corr after feature selection"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X[best_score] = scaler.fit_transform(X[best_score])

corr = df_feature.corr()
corr['label'].sort_values(ascending=False)[1:].to_frame().style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))

# best_score.append("label")
print(best_score)

best_score.append("label")
unique_best_feature = set(best_score)
best_feature_df = df_feature[list(unique_best_feature)]
best_feature_df.head()

best_corr =  best_feature_df.corr()
corr['label'].sort_values(ascending=False)[1:].to_frame().style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))

"""# 4.Bài toán xác định mạng có bị nhiễm mã độc không - Binary Classification
(RF, SVM, kNN, DT, NB, XGB)

Bài toán xác định mạng có bị nhiễm mã độc IoT hay không
"""

# Dùng df_balance - bộ dữ liêu đã đc làm sạch và cắt giảm mẫu để tránh 
df_balance.isna().sum()

df_balance["resp_bytes"]
df_balance["resp_bytes"] = df_balance["resp_bytes"].replace(np.nan, 0)

df_train = df_balance
time_usage = {}

df_train.info()

df_train

"""# Train full feature"""

X = df_train.drop(["label"], axis=1)

Y = df_train['label']

X

Y

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=10, test_size=0.2)

labels = ["Train set", "Test set"]
botnet_means = [Y_train.value_counts()[1], Y_test.value_counts()[1]]
normal_means = [Y_train.value_counts()[0], Y_test.value_counts()[0]]

x = np.arange(len(labels))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize=(9, 6))
rects1 = ax.bar(x - width/2, botnet_means, width, label='Botnet flow')
rects2 = ax.bar(x + width/2, normal_means, width, label='Normal flow')


for bar in ax.patches:
  # The text annotation for each bar should be its height.
  bar_value = bar.get_height()
  text = f'{bar_value:,}'
  text_x = bar.get_x() + bar.get_width() / 2
  text_y = bar.get_y() + bar_value

  bar_color = bar.get_facecolor()
  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,
          size=12)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel("Number of Sample")
ax.set_title("Number of flow in each group")
plt.xticks(x, labels=["Train set", "Test set"])

ax.legend()


fig.tight_layout()

plt.show()

"""###1. SVM

"""

plot_time=[]

X_train

# split train and file test is 7:3 hoac linear
from sklearn.svm import LinearSVC
SVM_classifier = LinearSVC(tol=0.0001, verbose=True)

# Up cache size to 1500
start = time.time()
SVM_classifier.fit(X_train, Y_train)
print()

y_SVM_pred = SVM_classifier.predict(X_test)
# y_NB_pred = clf.fit(X_NB_train, Y_NB_train).predict(X_NB_test)

y_pred = np.array(y_SVM_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_SVM_pred)
SVM_accuracy_score = float(correct)/y_true.shape[0]
print("SVM accuracy score: ",SVM_accuracy_score)
end = time.time()
SVM_time = end- start

time_usage.update({"SVM" : SVM_time})

print(y_SVM_pred)
print()

print("Classifiction SVM:")
print(classification_report(Y_test, y_SVM_pred))

precision_SVM = precision_score(Y_test,y_SVM_pred,average="macro")
recall_score_SVM = recall_score(Y_test,y_SVM_pred,average="macro")
f1_score_SVM = f1_score(Y_test,y_SVM_pred,average="macro")

print('Precision: {:.5f}'.format(precision_score(Y_test,y_SVM_pred,average="macro")))
print('Recall: {:.5f}'.format(recall_score(Y_test,y_SVM_pred,average="macro")))
print('F1-score: {:.5f}\n'.format(f1_score(Y_test,y_SVM_pred,average="macro")))

cnf_matrix = metrics.confusion_matrix(Y_test, y_SVM_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix SVM', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""###2. Naive Bayes"""

start = time.time()
clf = GaussianNB()
clf.fit(X_train, Y_train)
print()


print('prediction:')
y_NB_pred = clf.predict(X_test)

y_pred = np.array(y_NB_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_NB_pred)
NB_accuracy_score = float(correct)/y_true.shape[0]

# NB_accuracy_score = clf.score(X_NB_test, Y_test)
print(NB_accuracy_score)
# print()
precision_NaiveBayes = precision_score(Y_test,y_NB_pred,average="macro")
recall_score_NaiveBayes = recall_score(Y_test,y_NB_pred,average="macro")
f1_score_NaiveBayes = f1_score(Y_test,y_NB_pred,average="macro")

end = time.time()
NB_time = end- start

time_usage.update({"Naive Bayes" : NB_time})

print(y_NB_pred)
print()

print("Classifiction Naive Bayes :")
print(classification_report(Y_test, y_NB_pred))

cnf_matrix = metrics.confusion_matrix(Y_test, y_NB_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix Naive Bayes', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""###3. Decision Tree"""

X_DT = X
Y_DT = Y
X_DT_train, X_DT_test, Y_DT_train, Y_DT_test = train_test_split(X_DT, Y_DT, random_state=20, test_size=0.3)

DT = DecisionTreeClassifier(max_depth=9, min_samples_leaf=2, min_samples_split=10, random_state=42)

start = time.time()
print('program start...')
print()

DT.fit(X_train, Y_train)
print()

print('prediction:')
y_DT_pred = DT.predict(X_test)
print(y_DT_pred)
print()

print('Accuracy Score:')
y_pred = np.array(y_DT_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_DT_pred)
DT_accuracy_score = float(correct)/y_true.shape[0]

print(DT_accuracy_score)

end = time.time()
print('program end...')
print()
DecisionTree_time = end-start
time_usage.update({"DecisionTree" : DecisionTree_time})
print('time cost: ')
print(end - start, 'seconds')

precision_DecisionTree = precision_score(Y_test,y_DT_pred,average="macro")
recall_score_DecisionTree = recall_score(Y_test,y_DT_pred,average="macro")
f1_score_DecisionTree = f1_score(Y_test,y_DT_pred,average="macro")

print("Decision Tree :")
print(classification_report(Y_test, y_DT_pred))

Y_test.tail()

y_DT_pred[-5:]

cnf_matrix = metrics.confusion_matrix(Y_test, y_DT_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(10, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix DT', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""###5. KNN
KNN give score = 0.8783 with k = 3.
Can make KNN faster when add param (algorithm='kd_tree')
but it becomes inefficient as grows very large: this is one manifestation of the so-called “curse of dimensionality”
"""

from sklearn.neighbors import KNeighborsClassifier

X_kNN = X
Y_kNN = Y
X_kNN_train, X_kNN_test, Y_kNN_train, Y_kNN_test = train_test_split(X_kNN, Y_kNN, random_state=20, test_size=0.3)

"""Find best k for KNN"""

# model = KNeighborsClassifier(n_neighbors=2,n_jobs=1)
# model.fit(X_kNN_train, Y_kNN_train)

# accuracy = accuracy_score(model.predict(X_kNN_test), Y_kNN_test)
# print(accuracy)
# n_neighbors = np.array([7,8,9,10,12,15,20])
# param_grid = dict(n_neighbors=n_neighbors)
# grid = GridSearchCV(estimator=model, param_grid=param_grid)
# grid.fit(X_kNN_train, Y_kNN_train)
# print(grid.best_score_)
# print(grid.best_estimator_.n_neighbors)

#find best k = 7
knn = KNeighborsClassifier(n_neighbors = 7)

start = time.time()
print('Knn start...')
print()

knn.fit(X_train, Y_train)
print()

print('prediction:')
y_kNN_pred = knn.predict(X_test)


print('accuracy Score:')
y_pred = np.array(y_kNN_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_kNN_pred)
kNN_accuracy_score = float(correct)/y_true.shape[0]
print(kNN_accuracy_score)


precision_KNN = precision_score(Y_test,y_kNN_pred,average="macro")
recall_score_KNN = recall_score(Y_test,y_kNN_pred,average="macro")
f1_score_KNN = f1_score(Y_test,y_kNN_pred,average="macro")

end = time.time()
knn_time = end - start
time_usage.update({"KNN" : knn_time})

print('program end...')
print()
print('time cost: ')
print(knn_time, 'seconds')

print("k-nearest neighbors algorithm (k-NN)  :")
print(classification_report(Y_test, y_kNN_pred ))

cnf_matrix = metrics.confusion_matrix(Y_test, y_kNN_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(10, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix KNN', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""###6. Random Forest"""

X_RF = X
Y_RF = Y
X_RF_train, X_RF_test, Y_RF_train, Y_RF_test = train_test_split(X_RF, Y_RF, random_state=20, test_size=0.3)

RF = RandomForestClassifier(max_depth=2, random_state=42)
start = time.time()
print('Random Forest start...')
print()

RF.fit(X_train, Y_train)
print()

print('prediction:')
y_RF_pred = RF.predict(X_test)


print('Accuracy Score:')
y_pred = np.array(y_RF_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_RF_pred)
RF_accuracy_score = float(correct)/y_true.shape[0]
print(RF_accuracy_score)

precision_RandomForest = precision_score(Y_test,y_RF_pred,average="macro")
recall_score_RandomForest = recall_score(Y_test,y_RF_pred,average="macro")
f1_score_RandomForest = f1_score(Y_test,y_RF_pred,average="macro")

end = time.time()
RF_time = end- start
time_usage.update({"RandomForest" : RF_time})

print('program end...')
print()
print('time cost: ')
print(end - start, 'seconds')

print("Random Forest :")
print(classification_report(Y_test, y_RF_pred ))

cnf_matrix = metrics.confusion_matrix(Y_test, y_RF_pred)
class_names=[0,1]
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix Random Forest', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

Y_RF_test = pd.get_dummies(Y_RF_test)

n_classes = pd.get_dummies(Y_RF_test).shape[1]
print(n_classes)

# false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_RF_test, y_RF_pred)
# roc_auc = auc(false_positive_rate, true_positive_rate)
# roc_auc

# plt.figure(figsize=(10,10))
# plt.title('Receiver Operating Characteristic')
# plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)
# plt.legend(loc = 'lower right')
# plt.plot([0, 1], [0, 1],linestyle='--')
# plt.axis('tight')
# plt.ylabel('True Positive Rate')
# plt.xlabel('False Positive Rate')

# roc_auc_score()

# fpr = dict()
# tpr = dict()
# roc_auc = dict()

# for i in range(n_classes):
#     fpr[i], tpr[i], _ = roc_curve(Y_RF_test[:, i], score_RF[:, i])
#     roc_auc[i] = auc(fpr[i], tpr[i])

# # Compute micro-average ROC curve and ROC area
# fpr["micro"], tpr["micro"], _ = roc_curve(Y_RF_test.ravel(), score_RF.ravel())
# roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# yt# import scikitplot as sk_plt
# fig, ax = plt.subplots(figsize=(10, 6))
# fig.subplots_adjust(top=0.8, right=0.65)
# metrics.plot_roc_curve(X, Y.values.ravel(), title="ROC of RF", cmap='nipy_spectral', ax=ax)
# # plt.legend(bbox_to_anchor=(1.05, 1), loc='best', fontsize='medium')
# plt.legend(bbox_to_anchor=(1.05, 1), loc='best', fontsize='medium')

"""### XGB Classifier"""

from xgboost import XGBClassifier

X_xgb = X
Y_xgb = Y
X_xgb_train, X_xgb_test, Y_xgb_train, Y_xgb_test = train_test_split(X_xgb, Y_xgb, random_state=20, test_size=0.3)

xgb=XGBClassifier(learning_rate=0.1, n_estimator=300)
start = time.time()
print('Random Forest start...')

xgb.fit(X_train, Y_train)
print('prediction:')

y_xgb_pred = xgb.predict(X_test)


print('Accuracy Score:')
y_pred = np.array(y_xgb_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_xgb_pred)
XGB_accuracy_score = float(correct)/y_true.shape[0]
print(XGB_accuracy_score)

end = time.time()
xgb_time = end- start
time_usage.update({"XGB Classifier" : xgb_time})

print('program end...')
print()
print('time cost: ')
print(xgb_time, 'seconds')

precision_XGB = precision_score(Y_test,y_xgb_pred,average="macro")
recall_score_XGB = recall_score(Y_test,y_xgb_pred,average="macro")
f1_score_XGB = f1_score(Y_test,y_xgb_pred,average="macro")

print("XGB Classifier :")
print(classification_report(Y_test, y_xgb_pred))

cnf_matrix = metrics.confusion_matrix(Y_test, y_xgb_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix XGB Classifier', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""# Train best feature"""

df_process

# df_best_feature = df_process['ts', 'id.resp_p', 'orig_ip_bytes' ,'id.orig_p', 'duration', 'proto', "orig_pkts", 'history', "conn_state"]

df_best_feature

#chay day k can chay tren
X_bf = df_process[best_score]
Y_bf = df_process['label']

X_bf_train, X_bf_test, y_bf_train, y_bf_test = train_test_split(X_bf, Y_bf, random_state=42, test_size=0.2)

time_usage_bf = {}

"""## SVM"""

from sklearn.svm import LinearSVC
SVM_classifier = LinearSVC(tol=0.0001, verbose=True)

start = time.time()
SVM_classifier.fit(X_bf_train, y_bf_train)
print()

y_SVM_pred = SVM_classifier.predict(X_bf_test)
# y_NB_pred = clf.fit(X_NB_train, Y_NB_train).predict(X_NB_test)

y_pred = np.array(y_SVM_pred)
y_true = np.array(y_bf_test)
correct = np.sum(y_bf_test == y_SVM_pred)
SVM_accuracy_score_bf = float(correct)/y_true.shape[0]
print("SVM accuracy score: ",SVM_accuracy_score_bf)
end = time.time()
SVM_time = end- start

time_usage_bf.update({"SVM" : SVM_time})

print(y_SVM_pred)
print()

precision_SVM_bf = precision_score(y_bf_test,y_SVM_pred,average="macro")
recall_score_SVM_bf = recall_score(y_bf_test,y_SVM_pred,average="macro")
f1_score_SVM_bf = f1_score(y_bf_test,y_SVM_pred,average="macro")
print('Precision: {:.5f}'.format(precision_score(y_bf_test,y_SVM_pred,average="macro")))
print('Recall: {:.5f}'.format(recall_score(y_bf_test,y_SVM_pred,average="macro")))
print('F1-score: {:.5f}\n'.format(f1_score(y_bf_test,y_SVM_pred,average="macro")))

cnf_matrix = metrics.confusion_matrix(y_bf_test, y_SVM_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix best feature SVM', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""## Naive Bayes best feature"""

X_train, X_test, Y_train, Y_test = train_test_split(X_bf, Y_bf, random_state=42, test_size=0.2)

start = time.time()
clf = GaussianNB()
clf.fit(X_train, Y_train)
print()


print('prediction:')
y_NB_pred = clf.predict(X_test)

y_pred = np.array(y_NB_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_NB_pred)
NB_accuracy_score_bf = float(correct)/y_true.shape[0]

# NB_accuracy_score = clf.score(X_NB_test, Y_test)
print(NB_accuracy_score_bf)
# print()
precision_NaiveBayes_bf = precision_score(Y_test,y_NB_pred,average="macro")
recall_score_NaiveBayes_bf = recall_score(Y_test,y_NB_pred,average="macro")
f1_score_NaiveBayes_bf = f1_score(Y_test,y_NB_pred,average="macro")

end = time.time()
NB_time = end- start

time_usage_bf.update({"NaiveBayes" : NB_time})

print(y_NB_pred)
print()

cnf_matrix = metrics.confusion_matrix(Y_test, y_NB_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix Naive Bayes best feature', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""## Decision Tree bf"""

start = time.time()
print('program start...')
print()

DT.fit(X_train, Y_train)
print()

print('prediction:')
y_DT_pred = DT.predict(X_test)
print(y_DT_pred)
print()

print('Accuracy Score:')
y_pred = np.array(y_DT_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_DT_pred)
DT_accuracy_score_bf = float(correct)/y_true.shape[0]

print(DT_accuracy_score_bf)

precision_DecisionTree_bf = precision_score(Y_test,y_DT_pred,average="macro")
recall_score_DecisionTree_bf = recall_score(Y_test,y_DT_pred,average="macro")
f1_score_DecisionTree_bf = f1_score(Y_test,y_DT_pred,average="macro")

end = time.time()
print('program end...')
print()
DecisionTree_time = end-start
time_usage_bf.update({"DecisionTree" : DecisionTree_time})
print('time cost: ')
print(end - start, 'seconds')

cnf_matrix = metrics.confusion_matrix(Y_test, y_DT_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(10, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix DT best feature', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""## KNN bf"""

start = time.time()
print('Knn start...')
print()

knn.fit(X_train, Y_train)
print()

print('prediction:')
y_kNN_pred = knn.predict(X_test)


print('accuracy Score:')
y_pred = np.array(y_kNN_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_kNN_pred)
kNN_accuracy_score_bf = float(correct)/y_true.shape[0]
print(kNN_accuracy_score_bf)


precision_KNN_bf = precision_score(Y_test,y_kNN_pred,average="macro")
recall_score_KNN_bf = recall_score(Y_test,y_kNN_pred,average="macro")
f1_score_KNN_bf = f1_score(Y_test,y_kNN_pred,average="macro")

end = time.time()
knn_time = end - start
time_usage_bf.update({"KNN" : knn_time})

print('program end...')
print()
print('time cost: ')
print(knn_time, 'seconds')

cnf_matrix = metrics.confusion_matrix(Y_test, y_kNN_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(10, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix KNN best feature', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""## Random forest bf"""

RF = RandomForestClassifier(max_depth=2, random_state=42)
start = time.time()
print('Random Forest start...')
print()

RF.fit(X_train, Y_train)
print()

print('prediction:')
y_RF_pred = RF.predict(X_test)


print('Accuracy Score:')
y_pred = np.array(y_RF_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_RF_pred)
RF_accuracy_score_bf = float(correct)/y_true.shape[0]
print(RF_accuracy_score)

precision_RandomForest_bf = precision_score(Y_test,y_RF_pred,average="macro")
recall_score_RandomForest_bf = recall_score(Y_test,y_RF_pred,average="macro")
f1_score_RandomForest_bf = f1_score(Y_test,y_RF_pred,average="macro")

end = time.time()
RF_time = end- start
time_usage_bf.update({"RandomForest" : RF_time})

print('program end...')
print()
print('time cost: ')
print(end - start, 'seconds')

cnf_matrix = metrics.confusion_matrix(Y_test, y_RF_pred)
class_names=[0,1]
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix Random Forest best feature', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""## XGB bf"""

xgb=XGBClassifier(learning_rate=0.1, n_estimator=300)
start = time.time()
print('Random Forest start...')

xgb.fit(X_train, Y_train)
print('prediction:')

y_xgb_pred = xgb.predict(X_test)


print('Accuracy Score:')
y_pred = np.array(y_xgb_pred)
y_true = np.array(Y_test)
correct = np.sum(Y_test == y_xgb_pred)
XGB_accuracy_score_bf = float(correct)/y_true.shape[0]
print(XGB_accuracy_score_bf)

end = time.time()
xgb_time = end- start
time_usage_bf.update({"XGB Classifier" : xgb_time})

precision_XGB_bf = precision_score(Y_test,y_xgb_pred,average="macro")
recall_score_XGB_bf = recall_score(Y_test,y_xgb_pred,average="macro")
f1_score_XGB_bf = f1_score(Y_test,y_xgb_pred,average="macro")

print('program end...')
print()
print('time cost: ')
print(xgb_time, 'seconds')

cnf_matrix = metrics.confusion_matrix(Y_test, y_xgb_pred)
class_names=[0,1]
fig, ax = plt.subplots(figsize=(9, 6))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="coolwarm" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix XGB Classifier best feature', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""# PLOT Image

## accuaracy score
"""

scores = {
    "SVM": 0.391,
    "Naive Bayes": NB_accuracy_score,
    "DecisionTree": 0.989,
    "kNN": 0.994,
    "Random Forest": RF_accuracy_score,
    "XGB Classifier": XGB_accuracy_score,
    }
scores_best = {
    "SVM": 0.421,
    "Naive Bayes": NB_accuracy_score_bf,
    "DecisionTree": 0.991,
    "kNN": 0.993,
    "Random Forest": RF_accuracy_score_bf,
    "XGB Classifier": XGB_accuracy_score_bf,
    }

models = list(scores.keys())

values = list(scores.values())
values_bf = list(scores_best.values())

opacity = 0.4
bar_width = 0.35

plt.xlabel('Model')
plt.ylabel('Percent(%)')
fig,ax = plt.subplots(figsize=(9, 6))

plt.xticks(range(len(models)), labels=models)
bar1 = plt.bar(np.arange(len(values_bf)) + bar_width, values_bf, bar_width, align='center', alpha=opacity, color='b', label='best feature.')
bar2 = plt.bar(range(len(values)), values, bar_width, align='center', alpha=opacity, color='g', label='all feature')

# Add counts above the two bar graphs
for rect in bar2+ bar1:
    height = rect.get_height()
    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.3f}', ha='center', va='bottom')
ax.set_ylabel("Accuracy Score")
ax.set_title("Accuracy Score in different models")
plt.legend()
plt.tight_layout()
plt.show()

models = list(scores.keys())

values = list(scores.values())
values_bf = list(scores_best.values())

opacity = 0.4
bar_width = 0.35

# plt.xlabel('Model')
# plt.ylabel('Percent(%)')
fig,ax = plt.subplots(figsize=(9, 6))
# plt.xticks(range(len(models)),('[10-20)', '[20-30)', '[30-50)', '[50-70)','[70-90)', '[90-120)', ' [120 < )'), rotation=30)
plt.xticks(range(len(models)), labels=models)
bar1 = plt.bar(np.arange(len(values)) + bar_width, values, bar_width, align='center', alpha=opacity, color='g', label='all feature.')
bar2 = plt.bar(range(len(values_bf)), values_bf, bar_width, align='center', alpha=opacity, color='b', label='best feature')

# Add counts above the two bar graphs
for rect in bar2+ bar1:
    height = rect.get_height()
    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.3f}', ha='center', va='bottom')
ax.set_ylabel("Precision Score")
ax.set_title("Precision in different models")
plt.legend()
plt.tight_layout()
plt.show()

"""## precision, recall, f1"""

ML_models = ['SVM', 'NaiveBayes', 'DecisionTree', 'RandomForest', 'KNN', "XGB"]
precision = [precision_SVM, precision_NaiveBayes, precision_DecisionTree, precision_RandomForest, precision_KNN, precision_XGB]
f1_score = [f1_score_SVM, f1_score_NaiveBayes, f1_score_DecisionTree, f1_score_RandomForest, f1_score_KNN, f1_score_XGB]
recall_score = [recall_score_SVM, recall_score_NaiveBayes, recall_score_DecisionTree, recall_score_RandomForest, recall_score_KNN, recall_score_XGB]
# for model in ML_models:
#   precision += [f'precision_{model}']
#   f1_score += [f'f1_score_{model}']
#   recall_score += [f'recall_score_{model}']

print(precision)
print(f1_score)
print(recall_score)

precision_bf = [precision_SVM_bf, precision_NaiveBayes_bf, precision_DecisionTree_bf, precision_RandomForest_bf, precision_KNN_bf, precision_XGB_bf]
f1_score_bf = [f1_score_SVM_bf, f1_score_NaiveBayes_bf, f1_score_DecisionTree_bf, f1_score_RandomForest_bf, f1_score_KNN_bf, f1_score_XGB_bf]
recall_score_bf = [recall_score_SVM_bf, recall_score_NaiveBayes_bf, recall_score_DecisionTree_bf, recall_score_RandomForest_bf, recall_score_KNN_bf, recall_score_XGB_bf]

"""### precision"""

models = ML_models
values = [precision_SVM, precision_NaiveBayes, precision_DecisionTree, precision_RandomForest, precision_KNN, precision_XGB]
values_bf = [precision_SVM_bf, precision_NaiveBayes_bf, precision_DecisionTree_bf,precision_RandomForest_bf,  precision_KNN_bf, precision_XGB_bf]

opacity = 0.4
bar_width = 0.35

plt.xlabel('Model')
plt.ylabel('Percent(%)')
fig,ax = plt.subplots(figsize=(9, 6))

plt.xticks(range(len(models)), labels=models)
bar1 = plt.bar(np.arange(len(values_bf)) + bar_width , values_bf, bar_width, align='center', alpha=opacity, color='b', label='best feature.')
bar2 = plt.bar(range(len(values)), values, bar_width, align='center', alpha=opacity, color='g', label='all feature')

# Add counts above the two bar graphs
for rect in bar2+ bar1:
    height = rect.get_height()
    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.3f}', ha='center', va='bottom')
ax.set_ylabel("Precision")
ax.set_title("Precision in different models")
plt.legend()
plt.tight_layout

"""### recall"""

models = ML_models
values = [recall_score_SVM, recall_score_NaiveBayes, precision_DecisionTree, recall_score_RandomForest, recall_score_KNN, recall_score_XGB]
values_bf = [recall_score_SVM_bf, recall_score_NaiveBayes_bf, precision_DecisionTree_bf, recall_score_RandomForest_bf, recall_score_KNN_bf, recall_score_XGB_bf]

opacity = 0.4
bar_width = 0.35

plt.xlabel('Model')
plt.ylabel('Percent(%)')
fig,ax = plt.subplots(figsize=(9, 6))

plt.xticks(range(len(models)), labels=models)
bar1 = plt.bar(np.arange(len(values_bf)) + bar_width , values_bf, bar_width, align='center', alpha=opacity, color='b', label='best feature.')
bar2 = plt.bar(range(len(values)), values, bar_width, align='center', alpha=opacity, color='g', label='all feature')

# Add counts above the two bar graphs
for rect in bar2+ bar1:
    height = rect.get_height()
    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.3f}', ha='center', va='bottom')
ax.set_ylabel("Score")
ax.set_title("Recall score in different models")
plt.legend()
plt.tight_layout

"""### f1 score"""

models = ML_models
values = [f1_score_SVM, f1_score_NaiveBayes, f1_score_DecisionTree, f1_score_RandomForest, f1_score_KNN, f1_score_XGB]
values_bf = [f1_score_SVM_bf, f1_score_NaiveBayes_bf, f1_score_DecisionTree_bf, f1_score_RandomForest_bf, f1_score_KNN_bf, f1_score_XGB_bf]

opacity = 0.4
bar_width = 0.35

plt.xlabel('Model')
plt.ylabel('Percent(%)')
fig,ax = plt.subplots(figsize=(9, 6))

plt.xticks(range(len(models)), labels=models)
bar1 = plt.bar(np.arange(len(values_bf)) + bar_width , values_bf, bar_width, align='center', alpha=opacity, color='b', label='best feature.')
bar2 = plt.bar(range(len(values)), values, bar_width, align='center', alpha=opacity, color='g', label='all feature')

# Add counts above the two bar graphs
for rect in bar2+ bar1:
    height = rect.get_height()
    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.3f}', ha='center', va='bottom')
ax.set_ylabel("f1-score")
ax.set_title("F1 score in different models")
plt.legend()
plt.tight_layout

ind = np.arange(len(ML_models))
width = 0.25

fig, ax = plt.subplots(figsize=(18, 12))

precision_vals = precision
bar1 = plt.bar(ind, precision_vals, width, color = 'r')

f1_score_vals = f1_score
bar2 = plt.bar(ind+width, f1_score_vals, width, color='g')

recall_vals = recall_score
bar3 = plt.bar(ind+width*2, recall_vals, width, color = 'b')

plt.xlabel("Scores")
plt.ylabel('Model')
plt.title("Precision F1 and recall Score")

plt.xticks(ind+width,ML_models)
plt.legend( (bar1, bar2, bar3), ('Precision', 'F1-score', 'recall') )
plt.show()

# creating
models = list(time_usage.keys())
values = list(time_usage.values())

fig, ax = plt.subplots(figsize = (9, 6))

# creating the bar plot
plt.bar(models, values, color ='maroon',
        width = 0.3)
for bar in ax.patches:
  bar_value = round(bar.get_height(),2)

  text = f'{bar_value:,}'

  text_x = bar.get_x() + bar.get_width() / 2
  text_y = bar.get_y() + bar_value
  bar_color = bar.get_facecolor()
  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,
          size=12)
plt.xlabel("Model")
plt.ylabel("Time(s)")
plt.title("Time Usage in different models")
plt.show()

"""# BEST FEATURE ML TRAIN"""

X_bf = df_process[best_score]
Y_bf = df_process['label']

X_bf

Y_bf

X_bf_train, X_bf_test, y_bf_train, y_bf_test = train_test_split(X_bf, Y_bf, random_state=42, test_size=0.3)

training_algorithms = dict([
    # ('SVC_linear', Pipeline([('normalization', MinMaxScaler()), ('classifier', LinearSVC())])),
    ('GaussianNB', Pipeline([('normalization', StandardScaler()), ('classifier', GaussianNB())])),
    ('LogisticRegression', Pipeline([('normalization', StandardScaler()), ('classifier', LogisticRegression())])),
    ('DecisionTree', Pipeline([('normalization', StandardScaler()), ('classifier', DecisionTreeClassifier(max_depth=9, min_samples_leaf=2, min_samples_split=10, random_state=42))])),
    ('XGB', Pipeline([('normalization', StandardScaler()), ('classifier',XGBClassifier(learning_rate=0.1, n_estimator=300))])),
    ('RandomForest', Pipeline([('normalization', StandardScaler()), ('classifier', RandomForestClassifier(max_depth=2, random_state=42))])),
])

excute_times = {}
predict_score = {}
accuracy_score = {}
recall = {}
fscore = {}
for model_name in training_algorithms.keys():
  print("=====> Train " + model_name + " . . .")
  start_time = time.time()
  model = training_algorithms[model_name]

  y_bf_pred = model.fit(X_bf_train, y_bf_train)

  y_pred = np.array(y_bf_pred)
  y_true = np.array(y_bf_test)
  correct = np.sum(y_bf_test == y_bf_pred)
  accuracy_score[f'{model_name}'] = float(correct)/y_true.shape[0]
  
  predict_score[f'{model_name}'] = model.predict(X_bf_test)
  # recall_score[f'{model_name}'] = recall_score(X_bf_test, y_bf_pred, average='macro')
  # f1_score[f'{model_name}'] = f1_score(X_bf_test, y_bf_pred, average='macro')

  excute_times[f'{model_name}'] = time.time() - start_time

models = list(accuracy_score.keys())
values = list(accuracy_score.values())

fig = plt.figure(figsize = (12, 6))

# creating the bar plot
plt.bar(models, values, color ='purple',
        width = 0.4)

plt.xlabel("Model")
plt.ylabel("Percent(%)")
plt.title("Acurancy Score in different models")
plt.show()



"""# 5. Detect type of attack(multiple)

## Prepare dataset
"""

df_process

df_preprocess_multi = df_dataset

df_preprocess_multi

# df_multiple_att = data_CC

# C%C - C&C-HeartBeat - C&C-FileDownload - C&C-Torii - C&C-Mirai - 'C&C-HeartBeat-FileDownload' : 1
# Attack : 2
# DDos : 3
# Okiru : 4
# PartOfAHorizontalPortScan : 5

df_preprocess_multi.loc[(df_preprocess_multi.label == 'PartOfAHorizontalPortScan'), 'label'] = 5
df_preprocess_multi.loc[(df_preprocess_multi.label == 'Okiru'), 'label'] = 4
df_preprocess_multi.loc[(df_preprocess_multi.label == 'Okiru-Attack'), 'label'] = 4
df_preprocess_multi.loc[(df_preprocess_multi.label == 'Benign'), 'label'] = 0
df_preprocess_multi.loc[(df_preprocess_multi.label == 'DDoS'), 'label'] = 3
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'Attack'), 'label'] = 2
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C-HeartBeat'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C-FileDownload'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C-Torii'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C-HeartBeat-FileDownload'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'FileDownload'), 'label'] = 1
df_preprocess_multi.loc[(df_preprocess_multi.label == 'C&C-Mirai'), 'label'] = 1

df_preprocess_multi

df_preprocess["label"] = df_preprocess["label"].astype(str).astype(float)

# df_preprocess = df_preprocess.drop([], axis=1)

cat_cols = []
for col in df_preprocess_multi.select_dtypes(include='object'):
  cat_cols.append(col)
num_cols = []
for col in df_preprocess_multi.select_dtypes(exclude='object'):
  num_cols.append(col)
print(num_cols)
print('----')
print(cat_cols)

df_preprocess["label"] = df_preprocess["label"].astype(str).astype(float)

"""### Prepare cach 1 (k chay cach 1 cho ket qua yeu hon)"""

label_encoder = LabelEncoder()
# onehot_encoder = OneHotEncoder(sparse=False, categories=auto)
#convert from object to str first then to int
# df_preprocess_multi['ts'] = pd.to_numeric(df_preprocess_multi['ts'], errors='coerce')
# df_preprocess_multi['ts'] = df_preprocess_multi['ts'].astype(str).astype(float)

#df_process['proto'] = df_process['proto'].replace('-', '90')
# df_preprocess_multi['proto'] = df_preprocess_multi['proto'].str.replace('-', "0")
# df_preprocess_multi['proto'] = df_preprocess_multi['proto'].astype(str)
# df_preprocess_multi['proto'] = label_encoder.fit_transform(df_preprocess_multi['proto'])

# df_preprocess_multi['service'] = df_preprocess_multi['service'].str.replace('-', '0')
#df_process['service'] = df_process['service'].replace('-', ' ')
# df_preprocess_multi['service'] = df_preprocess_multi['service'].astype(str)
# df_preprocess_multi['service'] = label_encoder.fit_transform(df_preprocess_multi['service'])

df_preprocess_multi['history'] = df_preprocess_multi['history'].str.replace('-', '0')
df_preprocess_multi['history'] = df_preprocess_multi['history'].astype(str)
df_preprocess_multi['history'] = label_encoder.fit_transform(df_preprocess_multi['history'])

df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('-', 0)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('S0', 1)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('S1', 2)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('S2', 3)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('S3', 4)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('SF', 5)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('REJ', 6)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('RSTO', 7)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('RSTR', 8)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('RSTOS0', 9)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('RSTRH', 10)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('SH', 11)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('SHR', 12)
df_preprocess_multi['conn_state'] = df_preprocess_multi['conn_state'].replace('OTH', 13)

#df_process['conn_state'] = df_process['conn_state'].astype(str)
#df_process['conn_state'] = encoder.fit_transform(df_process['conn_state'])
df_preprocess_multi['conn_state'].dtypes

proto_mapping = {
                  '-': 0,
                  'icmp': 1,
                  'tcp': 2,
                  'udp': 3,
                 }
for key, value in proto_mapping.items():
  df_preprocess_multi['proto'] = df_preprocess_multi['proto'].replace(key, value)


service_mapping = {
                    "-": 0,
                    "dhcp": 1,
                    "dns": 2,
                    "http": 3,
                    "ssh": 4,
                    "ssl": 5,
                    "irc": 6
                  }
for key, value in proto_mapping.items():
  df_preprocess_multi['proto'] = df_preprocess_multi['proto'].replace(key, value)

df_preprocess_multi['label'].value_counts()

# df_preprocess_multi['label'] = pd.to_numeric(df_preprocess_multi['label'], errors='coerce')
# df_preprocess_multi['label'] = df_preprocess_multi['label'].astype(str).astype(float)

df_preprocess_multi['label'].dtypes

df_preprocess_multi

df_preprocess_multi.info()

df_preprocess_multi["resp_bytes"]
df_preprocess_multi["resp_bytes"] = df_preprocess_multi["resp_bytes"].replace(np.nan, 0)

df_preprocess_multi.isna().sum()

df_preprocess_multi["label"] = df_preprocess_multi["label"].replace(np.nan, 1)

"""### Prepare cach2"""

df_process = df_preprocess

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df_process['proto'] = df_process['proto'].str.replace('-', ' ')
df_process['proto'] = df_process['proto'].astype(str)
df_process['proto'] = encoder.fit_transform(df_process['proto'])

df_process['service'] = df_process['service'].str.replace('-', ' ')
df_process['service'] = df_process['service'].astype(str)
df_process['service'] = encoder.fit_transform(df_process['service'])

df_process['conn_state'] = df_process['conn_state'].str.replace('-', ' ')
df_process['conn_state'] = df_process['conn_state'].astype(str)
df_process['conn_state'] = encoder.fit_transform(df_process['conn_state'])

df_process['history'] = df_process['history'].str.replace('-', ' ')
df_process['history'] = df_process['history'].astype(str)
df_process['history'] = encoder.fit_transform(df_process['history'])

df_process['duration'] = df_process['duration'].str.replace('S0', ' ')
df_process['duration'] = df_process['duration'].str.replace('-', ' ')
df_process['duration'] = pd.to_numeric(df_process['duration'], errors='coerce')
df_process['duration'] = df_process['duration'].astype(str).astype(float)

# df_process['orig_bytes'] = df_process['orig_bytes'].str.replace('-', ' ')
df_process['orig_bytes'] = pd.to_numeric(df_process['orig_bytes'], errors='coerce')

df_process['resp_bytes'] = df_process['resp_bytes'].str.replace('-', ' ')
df_process['resp_bytes'] = pd.to_numeric(df_process['resp_bytes'], errors='coerce')

# df_process['id.orig_h'] = df_process['id.orig_h'].str.replace('-', ' ')
# df_process['id.resp_h'] = df_process['id.resp_h'].str.replace('-', ' ')

# df_process['id.orig_h'] = df_process['id.orig_h'].astype(str)
# df_process['id.orig_h'] = encoder.fit_transform(df_process['id.orig_h'])

# df_process['id.resp_h'] = df_process['id.resp_h'].astype(str)
# df_process['id.resp_h'] = encoder.fit_transform(df_process['id.resp_h'])

df_process[['duration','orig_bytes','resp_bytes']] = df_process[['duration','orig_bytes','resp_bytes']].fillna(value=df_process[['duration','orig_bytes','resp_bytes']].mean())

"""## Model"""

X_mul = df_preprocess_multi[best_score]
Y_mul = df_preprocess_multi["label"]

# cat_cols.append("label")
# X = df_preprocess_multi.drop(columns=cat_cols, axis=1)
# Y = df_preprocess_multi['label']

X_mul

Y_mul

X_mul_train, X_mul_test, Y_mul_train, Y_mul_test = train_test_split(X_mul, Y_mul, random_state=10, test_size=0.2)

from sklearn.metrics import f1_score

time_usage_mul = {}

"""### Decision Tree mul"""

DT = DecisionTreeClassifier(max_depth=9, min_samples_leaf=2, min_samples_split=10, random_state=42)

start = time.time()
print('program start...')
print()

DT.fit(X_mul_train, Y_mul_train)
print()

print('prediction:')
y_DT_pred = DT.predict(X_mul_test)
print(y_DT_pred)
print()

print('Accuracy Score:')
y_pred = np.array(y_DT_pred)
y_true = np.array(Y_mul_test)
correct = np.sum(Y_mul_test == y_DT_pred)
DT_accuracy_score_mul = float(correct)/y_true.shape[0]

print(DT_accuracy_score_mul)

precision_DecisionTree_mul = precision_score(Y_mul_test,y_DT_pred,average="macro")
recall_score_DecisionTree_mul = recall_score(Y_mul_test,y_DT_pred,average="macro")
f1_score_DecisionTree_mul = f1_score(Y_mul_test,y_DT_pred,average="macro")

end = time.time()
print('program end...')
print()
DecisionTree_time_mul = end-start
time_usage_mul.update({"DecisionTree" : DecisionTree_time_mul})
print('time cost: ')
print(end - start, 'seconds')

print("Decision Tree multiple classifier :")
print(classification_report(Y_mul_test, y_DT_pred))

"""### Random Forest mul"""

RF = RandomForestClassifier(max_depth=2, random_state=42)

start = time.time()
print('Random Forest start...')
print()

RF.fit(X_mul_train, Y_mul_train)
print()

print('prediction:')
y_RF_pred = RF.predict(X_mul_test)


print('Accuracy Score:')
y_pred = np.array(y_RF_pred)
y_true = np.array(Y_mul_test)
correct = np.sum(Y_mul_test == y_RF_pred)
RF_accuracy_score_mul = float(correct)/y_true.shape[0]
print(RF_accuracy_score_mul)

precision_RandomForest_mul = precision_score(Y_mul_test,y_RF_pred,average="macro")
recall_score_RandomForest_mul = recall_score(Y_mul_test,y_RF_pred,average="macro")
f1_score_RandomForest_mul = f1_score(Y_mul_test,y_RF_pred,average="macro")

end = time.time()
RF_time_mul = end- start
time_usage_mul.update({"RandomForest" : RF_time_mul})

print('program end...')
print()
print('time cost: ')
print(end - start, 'seconds')

print("Random Forest multiple classifier :")
print(classification_report(Y_mul_test, y_RF_pred))

"""### XGB mul"""

from xgboost import XGBClassifier
xgb=XGBClassifier(learning_rate=0.1, n_estimator=300)
start = time.time()
print('Extremely Gradient Boost start...')

xgb.fit(X_mul_train, Y_mul_train)
print('prediction:')

y_xgb_pred = xgb.predict(X_mul_test)


print('Accuracy Score:')
y_pred = np.array(y_xgb_pred)
y_true = np.array(Y_mul_test)
correct = np.sum(Y_mul_test == y_xgb_pred)
XGB_accuracy_score_mul = float(correct)/y_true.shape[0]
print(XGB_accuracy_score_mul)

end = time.time()
xgb_time_mul = end- start
time_usage_mul.update({"XGB Classifier" : xgb_time_mul})

precision_XGB_mul = precision_score(Y_mul_test,y_xgb_pred,average="macro")
recall_score_XGB_mul = recall_score(Y_mul_test,y_xgb_pred,average="macro")
f1_score_XGB_mul = f1_score(Y_mul_test,y_xgb_pred,average="macro")

print('program end...')
print()
print('time cost: ')
print(xgb_time_mul, 'seconds')

print("Extremely Gradient Boost multiple classifier :")
print(classification_report(Y_mul_test, y_xgb_pred))

"""### KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 7)

start = time.time()
print('Knn start...')
print()

knn.fit(X_mul_train, Y_mul_train)
print()

print('prediction:')
y_kNN_pred = knn.predict(X_mul_test)


print('accuracy Score:')
y_pred = np.array(y_kNN_pred)
y_true = np.array(Y_mul_test)
correct = np.sum(Y_mul_test == y_kNN_pred)
kNN_accuracy_score_mul = float(correct)/y_true.shape[0]
print(kNN_accuracy_score_mul)


precision_KNN_mul = precision_score(Y_mul_test,y_kNN_pred,average="macro")
recall_score_KNN_mul = recall_score(Y_mul_test,y_kNN_pred,average="macro")
f1_score_KNN_mul = f1_score(Y_mul_test,y_kNN_pred,average="macro")

end = time.time()
knn_time_mul = end - start
time_usage_mul.update({"KNN" : knn_time_mul})

print('program end...')
print()
print('time cost: ')
print(knn_time_mul, 'seconds')

print("KNN multiple classifier :")
print(classification_report(Y_mul_test, y_kNN_pred))

"""## PLot Image Multiple Classifier"""

models = ["DecisionTree", "KNN", "RandomForest", "XGB"]
values = [DT_accuracy_score_mul, kNN_accuracy_score_mul,RF_accuracy_score_mul,XGB_accuracy_score_mul]

fig, ax = plt.subplots(figsize = (8, 6))

# creating the bar plot
plt.bar(models, values, color ='green',
        width = 0.25)
for bar in ax.patches:
  # The text annotation for each bar should be its height.
  bar_value = round(bar.get_height(),4)
  # Format the text with commas to separate thousands. You can do
  # any type of formatting here though.
  text = f'{bar_value:,}'
  text_x = bar.get_x() + bar.get_width() / 2
  text_y = bar.get_y() + bar_value
  bar_color = bar.get_facecolor()
  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,
          size=12)
  
plt.xlabel("Model")
plt.ylabel("Score")
plt.title("Accurancy Score in different models")
plt.show()

models = list(time_usage_mul.keys())
values = list(time_usage_mul.values())


fig, ax = plt.subplots(figsize = (8, 6))

# creating the bar plot
plt.bar(models, values, color ='green',
        width = 0.25)
for bar in ax.patches:
  # The text annotation for each bar should be its height.
  bar_value = int(bar.get_height())
  # Format the text with commas to separate thousands. You can do
  # any type of formatting here though.
  text = f'{bar_value:,}'
  text_x = bar.get_x() + bar.get_width() / 2
  text_y = bar.get_y() + bar_value
  bar_color = bar.get_facecolor()
  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,
          size=12)
plt.xlabel("Model")
plt.ylabel("Time(s)")
plt.title("Time Usage in different models")
plt.show()

"""### Multiple clasification precision, recall, f1 score"""

ML_models_mul = ["DecisionTree", "KNN", "RandomForest", "XGB"]
precision_mul = [precision_DecisionTree_mul, precision_KNN_mul,precision_RandomForest_mul,precision_XGB_mul]
f1_score_mul = [recall_score_DecisionTree_mul, recall_score_KNN_mul,recall_score_RandomForest_mul, recall_score_XGB_mul]
recall_score_mul = [f1_score_DecisionTree_mul, f1_score_KNN_mul,f1_score_RandomForest_mul,f1_score_XGB_mul]

labels = ML_models_mul


x = np.arange(len(labels))  # the label models
width = 0.2  # the width of the bars

fig, ax = plt.subplots(figsize=(12, 8))
rects1 = ax.bar(x - width, precision_mul, width, label='precision')
rects2 = ax.bar(x , f1_score_mul, width, label='f1-score')
rects2 = ax.bar(x + width, recall_score_mul, width, label='recall')



for bar in ax.patches:
  # The text annotation for each bar should be its height.
  bar_value = bar.get_height()
  # Format the text with commas to separate thousands. You can do
  # any type of formatting here though.
  text = f'{round(bar_value,3):,}'
  # This will give the middle of each bar on the x-axis.
  text_x = bar.get_x() + bar.get_width() / 2
  # get_y() is where the bar starts so we add the height to it.
  text_y = bar.get_y() + bar_value
  # If we want the text to be the same color as the bar, we can
  # get the color like so:
  bar_color = bar.get_facecolor()
  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,
          size=12)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel("Score")
ax.set_title("Precision F1-score and recall-score of multiple classification")
plt.xticks(x, labels=ML_models_mul)

ax.legend()


fig.tight_layout()

plt.show()